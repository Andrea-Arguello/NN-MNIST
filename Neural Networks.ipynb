{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universidad del Valle de Guatemala\n",
    "<br>\n",
    "Inteligencia Artificial\n",
    "<br>\n",
    "Andrea Argüello 17801\n",
    "<br>\n",
    "<center><b><font size=\"6\">Laboratorio 2: Redes Neuronales</font></b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neural_networks as nn\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(r'./datasets/csv/fashion-mnist_train.csv')\n",
    "test = pd.read_csv(r'./datasets/csv/fashion-mnist_test.csv')\n",
    "\n",
    "#shuffled = df.sample(frac=1)\n",
    "\n",
    "#train, validate = np.split(shuffled, [50000])\n",
    "#train.to_csv(path_or_buf='./datasets/csv/fashion-mnist_train_50k.csv',index=False)\n",
    "#validate.to_csv(path_or_buf='./fashion-mnist_validate.csv',index=False)\n",
    "\n",
    "train = pd.read_csv(r'./datasets/csv/fashion-mnist_train_50k.csv')\n",
    "validate = pd.read_csv(r'./datasets/csv/fashion-mnist_validate.csv')\n",
    "\n",
    "NORMALIZE = 1000.0\n",
    "train_X = train[train.columns[~train.columns.isin(['label'])]].to_numpy() / NORMALIZE\n",
    "train_y = train[['label']].to_numpy()\n",
    "validate_X = validate[validate.columns[~validate.columns.isin(['label'])]].to_numpy() / NORMALIZE\n",
    "validate_y = validate[['label']].to_numpy()\n",
    "test_X = test[test.columns[~test.columns.isin(['label'])]].to_numpy() / NORMALIZE\n",
    "test_y = test[['label']].to_numpy()\n",
    "\n",
    "train_Y = (train_y == np.asarray(range(10))).astype(int)\n",
    "validate_Y = (validate_y == np.asarray(range(10))).astype(int)\n",
    "test_Y = (test_y == np.asarray(range(10))).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A setear variables\n",
    "NETWORK_ARCH = np.array([\n",
    "    784,\n",
    "    90, # ~(784*10)**0.5\n",
    "    10\n",
    "    ])\n",
    "theta_shapes = np.hstack((\n",
    "    NETWORK_ARCH[1:].reshape(len(NETWORK_ARCH)-1,1),\n",
    "    (NETWORK_ARCH[:-1]+1).reshape(len(NETWORK_ARCH)-1,1)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otras variables\n",
    "#flat_thetas = nn.flatten_list_of_arrays([\n",
    "#    np.random.rand(*theta_shape) for theta_shape in theta_shapes\n",
    "#])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_thetas = np.load('flat_thetas3.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización con scipy.optimize.minimize\n",
    "Esto se realizó en el archivo jupyter.py por aparte, ya que en jupyter se quedó en caché un error que persistió sin importar cuantas veces reiniciase el kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.load('result.npy') #este es result.x\n",
    "#result = result.reshape(len(result),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71560, 1)\n"
     ]
    }
   ],
   "source": [
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_value = lambda calc, real: (calc == real).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluando accuracy con el train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_thetas = nn.inflate_matrices(result, theta_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_a_train = nn.feed_forward(new_thetas, train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se obtienen los indices del porcentaje mas alto\n",
    "def accuracy(predictions, y):\n",
    "    pred = np.argmax(predictions[-1], axis = 1).reshape(len(predictions[-1]),1)\n",
    "    correct = ((pred==y) * 1).sum()\n",
    "        \n",
    "    print(\"Success: \", correct, \" out of \", len(predictions[-1]), \", i.e. \", correct * 100/len(predictions[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success:  44172  out of  50000 , i.e.  88.344\n"
     ]
    }
   ],
   "source": [
    "accuracy(new_a_train, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success:  8503  out of  10000 , i.e.  85.03\n"
     ]
    }
   ],
   "source": [
    "new_a_validate = nn.feed_forward(new_thetas, validate_X)\n",
    "accuracy(new_a_validate, validate_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis\n",
    "Veamos cuáles son las imágenes que más le cuesta identificar..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
